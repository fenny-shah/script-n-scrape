{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping IMDB top 250 (https://www.imdb.com/search/title/?groups=top_250) and leading webpages\n",
    "\n",
    "### Items Scrapped\n",
    "        • Movie Name\n",
    "        • Release Year\n",
    "        • Duaration\n",
    "        • IMDB Rating\n",
    "        • Posters\n",
    "        • Trailers\n",
    "        • Photos\n",
    "        • Genre(s)\n",
    "        • Description\n",
    "        • Director\n",
    "        • Writer\n",
    "        • Cast\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "#we will use random to specify sleep time in between the process.\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL= \"https://www.imdb.com/search/title/?groups=top_250\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(URL)\n",
    "time.sleep(0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load entire HTML content of the page by clicking the show more button 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException: No more pages to load.\n"
     ]
    }
   ],
   "source": [
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "driver.maximize_window()\n",
    "time.sleep(2) \n",
    "\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(2)\n",
    "for i in range(0, 4):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(random.random())\n",
    "    \n",
    "    try:\n",
    "        next_page_button = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'button.ipc-see-more__button[role=\"button\"]'))\n",
    "        )\n",
    "        body = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "        for k in range(0, 8):\n",
    "            body.send_keys(Keys.ARROW_UP)\n",
    "            time.sleep(random.random())   \n",
    "        next_page_button.click()\n",
    "        time.sleep(random.randint(3, 5))\n",
    "    except TimeoutException:\n",
    "        print(\"TimeoutException: No more pages to load.\")\n",
    "        break\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take out following information out of the rendered webpage:\n",
    "    \n",
    "    Links to individual webpages\n",
    "    Names of movies\n",
    "    Rating(IMDB)\n",
    "    Duration of movie\n",
    "    Movie Poster\n",
    "    Moive summary \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Initialize the WebDriver session\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Open the desired webpage\n",
    "    driver.get('http://example.com')\n",
    "\n",
    "    links=[]\n",
    "    movie_names=[]\n",
    "    ratings=[]\n",
    "    year=[]\n",
    "    duration=[]\n",
    "    poster=[]\n",
    "    summary=[]\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Ensure the session is active and find the elements\n",
    "    list_of_titles = driver.find_elements(By.CSS_SELECTOR, 'h3.ipc-title__text')\n",
    "    list_of_links = driver.find_elements(By.CSS_SELECTOR,'a.ipc-title-link-wrapper[tabindex=\"0\"]')\n",
    "    list_of_summary = driver.find_elements(By.CSS_SELECTOR,'div.ipc-html-content-inner-div')\n",
    "    list_of_ratings = driver.find_elements(By.CSS_SELECTOR,'span.ipc-rating-star--imdb[data-testid=\"ratingGroup--imdb-rating\"]')\n",
    "    raw_data = driver.find_elements(By.CSS_SELECTOR,'span.dli-title-metadata-item')\n",
    "    images = driver.find_elements(By.CSS_SELECTOR,'img.ipc-image[loading=\"lazy\"]')\n",
    "\n",
    "    # Process the list_of_titles as needed\n",
    "    for title in list_of_titles:\n",
    "        print(title.text)\n",
    "\n",
    "finally:\n",
    "    # Close the WebDriver session\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17076\\3183518457.py:10: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  replaced_m = (re.sub(\"^(\\d)*\\. \",'',m))\n"
     ]
    }
   ],
   "source": [
    "#now taking out all the necessary info from the raw data extracted using the find_elements method\n",
    "\n",
    "# Ensure the lists have at least 250 elements\n",
    "min_length = min(len(list_of_titles), len(list_of_links), len(list_of_summary), len(list_of_ratings), len(images))\n",
    "num_elements = min(250, min_length)\n",
    "\n",
    "for i in range(num_elements):\n",
    "    \n",
    "    m = list_of_titles[i].text\n",
    "    replaced_m = (re.sub(\"^(\\d)*\\. \",'',m))\n",
    "    final = re.sub(\"Recently viewed\",\"\",replaced_m)\n",
    "    movie_names.append(final)\n",
    "    \n",
    "    links.append(list_of_links[i].get_attribute(\"href\"))\n",
    "\n",
    "    summary.append(list_of_summary[i].text)\n",
    "\n",
    "    ratings.append(list_of_ratings[i].text[:3])\n",
    "\n",
    "    poster.append(images[i].get_attribute('src'))\n",
    "\n",
    "#Scraping release year and duration\n",
    "corrected_data=[]\n",
    "for i in range(len(raw_data)):\n",
    "    corrected_data.append(raw_data[i].text)\n",
    "corrected_data.insert(83,\"R\")\n",
    "corrected_data.insert(359,\"R\")\n",
    "corrected_data.insert(590,\"R\")\n",
    "for i in range(len(corrected_data)):\n",
    "    if (i%3==0):\n",
    "        year.append(corrected_data[i])\n",
    "    if (i%3==1):\n",
    "        duration.append(corrected_data[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() #it is important to close the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting info out of individual webpages using Beautiful Soup\n",
    "* Reviews(Critic only)\n",
    "* Trailer\n",
    "* Genre(s)\n",
    "* Cast\n",
    "* Director \n",
    "* Writer\n",
    "* Photos\n",
    "\n",
    "\n",
    ">Here I am using BS over find_elements method of selenium because BS supports multi-valued attributes.\n",
    "This makes searching the tree structure a lot more convenient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_genre=[]\n",
    "movie_directors=[]\n",
    "movie_writers=[]\n",
    "movie_cast=[]\n",
    "movie_lang=[]\n",
    "metascores=[]\n",
    "\n",
    "for URL in links:\n",
    "    \n",
    "    response = requests.get(\n",
    "    url='https://proxy.scrapeops.io/v1/',\n",
    "    params={\n",
    "        'api_key': 'f208ed04-171b-49a8-8f53-7c9d22f2c24f',\n",
    "        'url': URL, \n",
    "    },\n",
    "    )\n",
    "\n",
    "    soup = BeautifulSoup(response.text,features='html.parser')\n",
    "    \n",
    "    #initializing arrays\n",
    "    genre=[]\n",
    "    directors=[]\n",
    "    writers=[]\n",
    "    cast=[]\n",
    "    languages=[]\n",
    "    \n",
    "    #extracting genre\n",
    "    try:\n",
    "        genre_sup_div = soup.find('div',class_=\"sc-4e4cc5f9-10 jXLkbv\")\n",
    "        genre_div = genre_sup_div.find('div',class_=\"ipc-chip-list__scroller\")\n",
    "        genre_spans = genre_div.find_all('span')\n",
    "        for i in genre_spans:\n",
    "            genre.append(i.text)\n",
    "    except:\n",
    "        genre=[\"Movie\"] #There were no genre specified for one movie\n",
    "    \n",
    "    #extracting directors\n",
    "    try:\n",
    "        dir_ul = soup.find('ul',class_='ipc-inline-list ipc-inline-list--show-dividers ipc-inline-list--inline ipc-metadata-list-item__list-content baseAlt',attrs={'role':'presentation'})\n",
    "        dir_lis = dir_ul.find_all('li')\n",
    "        for i in dir_lis:\n",
    "            directors.append(i.text)\n",
    "    except:\n",
    "        pass #error will not occur, but still for the sake of doing it\n",
    "    \n",
    "    #extractinhg writers\n",
    "    try:\n",
    "        \n",
    "        big_ul = soup.find('ul',class_='ipc-metadata-list ipc-metadata-list--dividers-all sc-bfec09a1-8 bHYmJY ipc-metadata-list--base')\n",
    "        #here writer_uls also includes the ul of director. In the tree structure, the second ul is of writer.\n",
    "        writer_uls = big_ul.find_all('ul',class_='ipc-inline-list ipc-inline-list--show-dividers ipc-inline-list--inline ipc-metadata-list-item__list-content base')\n",
    "        writer_lis = writer_uls[1].find_all('li')   \n",
    "        for i in writer_lis:\n",
    "            writers.append(i.a.text)   \n",
    "    except:\n",
    "        pass#error will not occur, but still for the sake of doing it\n",
    "\n",
    "    #extracting cast\n",
    "    try:\n",
    "        cast_div = soup.find_all('div',class_=\"sc-bfec09a1-5 hNfYaW\",attrs={'data-testid':\"title-cast-item\"})\n",
    "        for i in cast_div:\n",
    "            a_tag_value = i.find('a',class_='sc-bfec09a1-1 gCQkeh').text\n",
    "            cast.append(a_tag_value)\n",
    "    except:\n",
    "        #error will not occur, but still for the sake of doing it\n",
    "        pass\n",
    "    #metascore\n",
    "    try:\n",
    "        metascore = soup.find('span',class_='metacritic-score-box').text\n",
    "    except:\n",
    "        metascore = \"N.A.\"\n",
    "    #now append to the greater list\n",
    "    movie_genre.append(genre)\n",
    "    movie_directors.append(directors)\n",
    "    movie_writers.append(writers)\n",
    "    movie_cast.append(cast)\n",
    "    movie_lang.append(languages)\n",
    "\n",
    "    if (metascore!=None):\n",
    "        metascores.append(metascore)\n",
    "    else:\n",
    "        #there are some movies for which meta score is not available\n",
    "        metascores.append(\"Has Not Been Calculated As Of Now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing in JSON data format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from textwrap import indent\n",
    "from matplotlib.font_manager import json_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here movie date is a list of dictionaries. We do so becuase there is a method to convert list of dictionaries to list of json objects.\n",
    "movie_data=[]\n",
    "\n",
    "#make a dictionary\n",
    "for i in range(len(movie_names)):\n",
    "    movie = {\n",
    "        \"index\" : i,\n",
    "        \"name\" : movie_names[i],\n",
    "        \"duration\" : duration[i],\n",
    "        \"rating\" : ratings[i],\n",
    "        \"release_year\" : year[i],\n",
    "        \"poster\" : poster[i],\n",
    "        \"summary\" : summary[i],\n",
    "        \"genre\" : movie_genre[i],\n",
    "        \"directors\" : movie_directors[i],\n",
    "        \"writers\" : movie_writers[i],\n",
    "        \"languages\" : movie_lang[i],\n",
    "        \"cast\" : movie_cast[i],\n",
    "        \"metascore\" : metascores[i],\n",
    "        \"imdb_page\" : links[i]\n",
    "    }\n",
    "    movie_data.append(movie)\n",
    "\n",
    "#convert dictionary to jason\n",
    "json_data = json.dumps(movie_data,indent=4)\n",
    "\n",
    "with open(\"movies.json\", \"w\") as outfile:\n",
    "  outfile.write(json_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
